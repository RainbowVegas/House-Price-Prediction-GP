{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQZL_grBP1qP",
        "outputId": "12ad9583-2627-45af-ca13-1b4349d5670c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "\n",
        "import catboost\n",
        "from catboost import CatBoostRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWXFvqnLP1qT"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv('datasets/train.csv')\n",
        "test_df = pd.read_csv('datasets/test.csv')\n",
        "\n",
        "train_df['Fence']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Solve for the more straightforward cases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# solve for more straightforward cases:\n",
        "# Alley (Grvl / Pave): \n",
        "most_common_alley = train_df['Alley'].mode()[0]\n",
        "train_df['Alley'].fillna(most_common_alley, inplace=True)\n",
        "\n",
        "# FireplaceQu\n",
        "train_df['FireplaceQu'].fillna(0, inplace=True)\n",
        "\n",
        "# Fence : \n",
        "# 281 non-null vs.\n",
        "# 1179 null -- drop\n",
        "train_df.drop(columns=['Fence'], inplace=True)\n",
        "\n",
        "# 7 non-null\n",
        "# 1453 null values\n",
        "#train_df.drop(columns=['PoolQC'], inplace=True)\n",
        "\n",
        "# Misc Feature : \n",
        "#train_df.drop(columns=['MiscFeature'], inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Now for the NaN Solving & model benchmarking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# NAN Solver : Really big block of spaghetti code\n",
        "\n",
        "# defined different methods of handling nans:\n",
        "# 1. set zero or 'vanilla' value\n",
        "# 2. fill median\n",
        "# 3. fill average\n",
        "# 4. drop\n",
        "# 5. interpolate\n",
        "\n",
        "from enum import Enum\n",
        "from itertools import product\n",
        "\n",
        "class NaNSolution(Enum):\n",
        "    DEFAULT = 1\n",
        "    MEDIAN = 2\n",
        "    MEAN = 3\n",
        "    DROP = 4\n",
        "    INTERPOLATE = 5\n",
        "\n",
        "\n",
        "# Create permutations\n",
        "permutations = product(NaNSolution, repeat=2)\n",
        "\n",
        "\n",
        "best_score = 0\n",
        "# remember best RSME result :\n",
        "least_rmse  = 9999999\n",
        "\n",
        "# best score NaN handler configuration :\n",
        "best_flag_LotFrontage = NaNSolution.DEFAULT\n",
        "best_flag_MasVnrType = NaNSolution.DEFAULT\n",
        "best_regressor = \"None\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qCnAJc49P1qX",
        "outputId": "0a893438-47d1-43b5-b79e-fecaf07fa8c0"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Iterate over permutations\n",
        "for permutation in permutations:\n",
        "    flag_LotFrontage, flag_MasVnrType = permutation\n",
        "\n",
        "    # reload data each permutation :\n",
        "    permute_train_df = train_df\n",
        "    permute_test_df = test_df\n",
        "\n",
        "    match flag_LotFrontage:\n",
        "        case NaNSolution.DEFAULT:\n",
        "        # default :\n",
        "            permute_train_df = permute_train_df['LotFrontage'].fillna(0) \n",
        "\n",
        "        case NaNSolution.MEDIAN:\n",
        "            permute_train_df = permute_train_df['LotFrontage'].fillna(permute_train_df.median())\n",
        "\n",
        "        case NaNSolution.MEAN:\n",
        "            permute_train_df = permute_train_df['LotFrontage'].fillna(permute_train_df.mean())\n",
        "\n",
        "        case NaNSolution.DROP:\n",
        "            permute_train_df = permute_train_df['LotFrontage'].dropna()\n",
        "\n",
        "        case NaNSolution.INTERPOLATE:\n",
        "            permute_train_df = permute_train_df['LotFrontage'].interpolate(method='linear', limit_direction='forward', axis=0)\n",
        "\n",
        "    # ['BrkFace', nan, 'Stone', 'BrkCmn']\n",
        "    # Since these are categorical, we'll use each case to be for filling in a different categorical value instead, since we can't clearly interpret categories in another way.  \n",
        "\n",
        "    match flag_MasVnrType:\n",
        "        case NaNSolution.DEFAULT:\n",
        "        # default :\n",
        "            permute_train_df = permute_train_df['MasVnrType'].fillna('BrkFace') \n",
        "\n",
        "        case NaNSolution.MEDIAN:\n",
        "            permute_train_df = permute_train_df['MasVnrType'].fillna('Stone')\n",
        "\n",
        "        case NaNSolution.MEAN:\n",
        "            permute_train_df = permute_train_df['MasVnrType'].fillna('BrkCmn')\n",
        "\n",
        "        case NaNSolution.DROP:\n",
        "            permute_train_df = permute_train_df['MasVnrType'].dropna()\n",
        "        \n",
        "        case NaNSolution.INTERPOLATE:\n",
        "        # this one doesn't really apply here.\n",
        "            continue\n",
        "\n",
        "    print(f'Trying LotFrontage NaN Handler... (1 - 5) : {flag_LotFrontage}')\n",
        "    print(f'Trying MasVnrType NaN Handler... (1 - 5)  : {flag_MasVnrType}')\n",
        "\n",
        "\n",
        "    # -------------------------------\n",
        "    X = permute_train_df.drop('SalePrice', axis=1)\n",
        "    y = permute_train_df['SalePrice']\n",
        "\n",
        "    # Lists of numerical and categorical columns\n",
        "    numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
        "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() < 10]\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    numerical_transformer = SimpleImputer(strategy='median') # NaNs should already by filled by this point (at least for most significant columns), hopefully.\n",
        "\n",
        "    # Preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Applying the preprocessing transformations\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split the preprocessed data into training and validation sets\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_preprocessed, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "    # -------------------------------\n",
        "    # Separating target variable and predictors\n",
        "    X = permute_train_df.drop('SalePrice', axis=1)\n",
        "    y = permute_train_df['SalePrice']\n",
        "\n",
        "    # Removing columns with too many missing values (>50% missing)\n",
        "    too_many_missing = [col for col in X.columns if X[col].isnull().sum() > X.shape[0] * 0.5]\n",
        "    X.drop(too_many_missing, axis=1, inplace=True)\n",
        "\n",
        "    # Lists of numerical and categorical columns\n",
        "    numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
        "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() < 10]\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    numerical_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "    # Preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Applying the preprocessing transformations\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split the preprocessed data into training and validation sets\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_preprocessed, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "\n",
        "      # -----------------------------------------------\n",
        "\n",
        "    # Define and train models - ** tune these systematically **\n",
        "    model_linear = LinearRegression()\n",
        "\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    from scipy.stats import uniform, randint\n",
        "    import xgboost as xgb\n",
        "\n",
        "    # Define the parameter distribution for each model\n",
        "    param_dist_ridge = {'alphas': uniform(0.01, 100)}\n",
        "    param_dist_lasso = {'alphas': uniform(0.0001, 10)}\n",
        "    param_dist_tree = {'max_depth': [None] + list(range(1, 31)), 'min_samples_split': randint(2, 11)}\n",
        "    param_dist_forest = {'n_estimators': randint(50, 200), 'max_depth': [None] + list(range(1, 31))}\n",
        "    param_dist_xgboost = {\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8, 9],\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "    'n_estimators': randint(50, 350),\n",
        "    'gamma': uniform(0, 1),\n",
        "    'subsample': [0.8, 0.9, 1.0],\n",
        "    }\n",
        "\n",
        "    param_dist_catboost = {\n",
        "        'depth': [3, 4, 5, 6, 7, 8, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "        'n_estimators': randint(50, 200),\n",
        "        'l2_leaf_reg': uniform(1, 10),\n",
        "        'subsample': [0.8, 0.9, 1.0],\n",
        "    }\n",
        "    \n",
        "\n",
        "    model_linear = LinearRegression()\n",
        "    model_ridge_cv = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5)\n",
        "    model_lasso_cv = LassoCV(alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10], cv=5, max_iter=10)\n",
        "    model_tree = DecisionTreeRegressor(random_state=0)\n",
        "    model_forest = RandomForestRegressor(n_estimators=6, random_state=0)\n",
        "    model_xgboost = xgb.XGBRegressor(random_state=0)\n",
        "    model_catboost = CatBoostRegressor(random_seed=0, verbose=False)\n",
        "\n",
        "    # Perform hyperparameter tuning with RandomizedSearchCV\n",
        "    random_search_xgboost = RandomizedSearchCV(\n",
        "        model_xgboost, param_distributions=param_dist_xgboost, n_iter=5, cv=5, random_state=0, n_jobs=-1)\n",
        "    random_search_catboost = RandomizedSearchCV(\n",
        "    model_catboost, param_distributions=param_dist_catboost, n_iter=150, cv=5, random_state=0, n_jobs=-1)\n",
        "\n",
        "\n",
        "    # Fit models\n",
        "    model_linear.fit(X_train, y_train)\n",
        "    model_ridge_cv.fit(X_train, y_train)\n",
        "    model_lasso_cv.fit(X_train, y_train)\n",
        "    model_tree.fit(X_train, y_train)\n",
        "    model_forest.fit(X_train, y_train)\n",
        "    random_search_xgboost.fit(X_train, y_train)\n",
        "    random_search_catboost.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    # Predictions and RMSE\n",
        "    predictions_linear = model_linear.predict(X_valid)\n",
        "    predictions_ridge = model_ridge_cv.predict(X_valid)\n",
        "    predictions_lasso = model_lasso_cv.predict(X_valid)\n",
        "    predictions_tree = model_tree.predict(X_valid)\n",
        "    predictions_forest = model_forest.predict(X_valid)\n",
        "    predictions_xgboost = random_search_xgboost.predict(X_valid)\n",
        "    predictions_catboost = random_search_catboost.predict(X_valid)\n",
        "\n",
        "    rmse_linear = sqrt(mean_squared_error(y_valid, predictions_linear))\n",
        "    rmse_ridge = sqrt(mean_squared_error(y_valid, predictions_ridge))\n",
        "    rmse_lasso = sqrt(mean_squared_error(y_valid, predictions_lasso))\n",
        "    rmse_tree = sqrt(mean_squared_error(y_valid, predictions_tree))\n",
        "    rmse_forest = sqrt(mean_squared_error(y_valid, predictions_forest))\n",
        "    rmse_xgboost = sqrt(mean_squared_error(y_valid, predictions_xgboost))\n",
        "    rmse_catboost = sqrt(mean_squared_error(y_valid, predictions_catboost))\n",
        "\n",
        "\n",
        "    print(f'Linear Regression RMSE: {rmse_linear}')\n",
        "    print(f'Ridge Regression RMSE: {rmse_ridge}')\n",
        "    print(f'Lasso Regression RMSE: {rmse_lasso}')\n",
        "    print(f'Decision Tree RMSE: {rmse_tree}')\n",
        "    print(f'Random Forest RMSE: {rmse_forest}')\n",
        "    print(f'XGBoost RMSE: {rmse_xgboost}')\n",
        "    print(f'CatBoost RMSE: {rmse_catboost}')\n",
        "    # Lastly, compare with best score, save configuration for best score.\n",
        "\n",
        "    new_high_score = False\n",
        "\n",
        "    if rmse_linear < least_rmse:\n",
        "        best_score = model_linear.score(X_train, y_train)\n",
        "        least_rmse = rmse_linear\n",
        "        best_regressor = \"Linear\"\n",
        "        new_high_score = True\n",
        "\n",
        "    if rmse_ridge  < least_rmse:\n",
        "        best_score = model_ridge_cv.score(X_train, y_train)\n",
        "        least_rmse = rmse_ridge\n",
        "        best_regressor = \"Ridge\"\n",
        "        new_high_score = True\n",
        "\n",
        "    if rmse_lasso  < least_rmse:\n",
        "        best_score = model_lasso_cv.score(X_train, y_train)\n",
        "        least_rmse = rmse_lasso\n",
        "        best_regressor = \"Lasso\"\n",
        "        new_high_score = True\n",
        "\n",
        "    if rmse_tree  < least_rmse:\n",
        "        best_score = model_tree.score(X_train, y_train)\n",
        "        least_rmse = rmse_tree\n",
        "        best_regressor = \"Tree\"\n",
        "        new_high_score = True\n",
        "\n",
        "\n",
        "    if rmse_forest  < least_rmse:\n",
        "        best_score = model_forest.score(X_train, y_train)\n",
        "        least_rmse = rmse_forest\n",
        "        best_regressor = \"Forest\"\n",
        "        new_high_score = True\n",
        "\n",
        "    if rmse_xgboost < least_rmse: \n",
        "        best_score = model_xgboost.score(X_train, y_train)\n",
        "        least_rmse = rmse_xgboost\n",
        "        best_regressor = \"XGBoost (Hyper Parameter Tuned)\"\n",
        "        new_high_score = True\n",
        "\n",
        "    if rmse_catboost < least_rmse: \n",
        "        best_score = model_catboost.score(X_train, y_train)\n",
        "        least_rmse = rmse_catboost\n",
        "        best_regressor = \"CatBoost (Hyper Parameter Tuned)\"\n",
        "        new_high_score = True\n",
        "\n",
        "    if new_high_score == True:\n",
        "        best_flag_LotFrontage = flag_LotFrontage\n",
        "    break\n",
        "\n",
        "\n",
        "\n",
        "# print result\n",
        "print(f'Best Model Score                     : {best_score}')\n",
        "print(f'Lowest RSME                          : {least_rmse}')\n",
        "print(f'Best Regressor                       : {best_regressor}')\n",
        "print(f'Best LotFrontage NaN Handler (1 - 5) : {best_flag_LotFrontage}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWSBSFAZc2pN",
        "outputId": "7e802048-8747-4c16-ff86-0678acd18868"
      },
      "outputs": [],
      "source": [
        "# (disabled testing code)\n",
        "if False: \n",
        "    permute_train_df = train_df\n",
        "    permute_test_df = test_df\n",
        "\n",
        "\n",
        "    X = permute_train_df.drop('SalePrice', axis=1)\n",
        "    y = permute_train_df['SalePrice']\n",
        "\n",
        "    # Lists of numerical and categorical columns\n",
        "    numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
        "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() < 10]\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    numerical_transformer = SimpleImputer(strategy='median') # NaNs should already by filled by this point (at least for most significant columns), hopefully.\n",
        "\n",
        "    # Preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Applying the preprocessing transformations\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split the preprocessed data into training and validation sets\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_preprocessed, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "    # -------------------------------\n",
        "    # Separating target variable and predictors\n",
        "    X = permute_train_df.drop('SalePrice', axis=1)\n",
        "    y = permute_train_df['SalePrice']\n",
        "\n",
        "    # Removing columns with too many missing values (>50% missing)\n",
        "    too_many_missing = [col for col in X.columns if X[col].isnull().sum() > X.shape[0] * 0.5]\n",
        "    X.drop(too_many_missing, axis=1, inplace=True)\n",
        "\n",
        "    # Lists of numerical and categorical columns\n",
        "    numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
        "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() < 10]\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    numerical_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "    # Preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Applying the preprocessing transformations\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split the preprocessed data into training and validation sets\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_preprocessed, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # Define and train models - ** TODO : still need to tune these systematically **\n",
        "\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    from scipy.stats import uniform, randint\n",
        "\n",
        "    # Define the parameter distribution for each model\n",
        "    param_dist_forest = {'n_estimators': randint(10, 300), 'max_depth': [None] + list(range(1, 31))}\n",
        "    rand_forest = RandomizedSearchCV(RandomForestRegressor(random_state=0), param_distributions=param_dist_forest, n_iter=55, cv=5)\n",
        "    rand_forest.fit(X_train, y_train)\n",
        "    predictions_forest = rand_forest.predict(X_valid)\n",
        "    rmse_forest = sqrt(mean_squared_error(y_valid, predictions_forest))\n",
        "    print(f'Forest Regression RMSE: {rmse_forest}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Testing CatBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# testing just Cat Boost: \n",
        "if False:\n",
        "    # also trying iterative impute\n",
        "    from sklearn.impute import IterativeImputer\n",
        "    from sklearn.experimental import enable_iterative_imputer\n",
        "\n",
        "    permute_train_df = train_df\n",
        "    permute_test_df = test_df\n",
        "\n",
        "    X = permute_train_df.drop('SalePrice', axis=1)\n",
        "    y = permute_train_df['SalePrice']\n",
        "\n",
        "    # Lists of numerical and categorical columns\n",
        "    numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
        "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() < 10]\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    #numerical_transformer = SimpleImputer(strategy='median') # NaNs should already by filled by this point (at least for most significant columns), hopefully.\n",
        "    numerical_transformer = IterativeImputer(max_iter=10, random_state=0)\n",
        "\n",
        "\n",
        "    # Preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', IterativeImputer(max_iter=10, random_state=0, strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Applying the preprocessing transformations\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split the preprocessed data into training and validation sets\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_preprocessed, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "    # -------------------------------\n",
        "    # Separating target variable and predictors\n",
        "    X = permute_train_df.drop('SalePrice', axis=1)\n",
        "    y = permute_train_df['SalePrice']\n",
        "\n",
        "    # Removing columns with too many missing values (>50% missing)\n",
        "    too_many_missing = [col for col in X.columns if X[col].isnull().sum() > X.shape[0] * 0.5]\n",
        "    X.drop(too_many_missing, axis=1, inplace=True)\n",
        "\n",
        "    # Lists of numerical and categorical columns\n",
        "    numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
        "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() < 10]\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    numerical_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "    # Preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Applying the preprocessing transformations\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split the preprocessed data into training and validation sets\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_preprocessed, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # Define and train models - ** TODO : still need to tune these systematically **\n",
        "\n",
        "    from sklearn.model_selection import RandomizedSearchCV\n",
        "    from scipy.stats import uniform, randint\n",
        "\n",
        "    param_dist_catboost = {\n",
        "        'depth': [3, 4, 5, 6, 7, 8, 9],\n",
        "        'learning_rate': [0.01, 0.1, 0.2, 0.3],\n",
        "        'n_estimators': randint(50, 200),\n",
        "        'l2_leaf_reg': uniform(1, 10),\n",
        "        'subsample': [0.8, 0.9, 1.0],\n",
        "    }\n",
        "    model_catboost = CatBoostRegressor(random_seed=0, verbose=False)\n",
        "    random_search_catboost = RandomizedSearchCV(\n",
        "    model_catboost, param_distributions=param_dist_catboost, n_iter=200, cv=5, random_state=0, n_jobs=-1)\n",
        "    random_search_catboost.fit(X_train, y_train)\n",
        "    predictions_catboost = random_search_catboost.predict(X_valid)\n",
        "    rmse_catboost = sqrt(mean_squared_error(y_valid, predictions_catboost))\n",
        "    print(f'CatBoost RMSE: {rmse_catboost}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Output CSV Submission File"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWUaDWRe6oVW"
      },
      "outputs": [],
      "source": [
        "# Assuming you've chosen the best model based on RMSE\n",
        "# For example, let's say the ____ was the best\n",
        "\n",
        "# now, going to try catboost as best:\n",
        "random_search_catboost = RandomizedSearchCV(\n",
        "model_catboost, param_distributions=param_dist_catboost, n_iter=500, cv=5, random_state=0, n_jobs=-1)\n",
        "random_search_catboost.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "best_model = random_search_catboost.best_estimator_\n",
        "train_df = permute_train_df\n",
        "test_df = permute_test_df\n",
        "# Re-train the model on the entire training dataset\n",
        "# Make sure 'train_df' is your entire training dataset\n",
        "X_full = preprocessor.fit_transform(train_df.drop('SalePrice', axis=1))  # Preprocess features\n",
        "y_full = train_df['SalePrice']  # Target variable\n",
        "best_model.fit(X_full, y_full)\n",
        "\n",
        "# Preprocess the test dataset\n",
        "# Apply the same preprocessing steps used for the training dataset\n",
        "test_preprocessed = preprocessor.transform(test_df)  # 'test_df' should be your raw test dataset\n",
        "\n",
        "# Predict house prices on the test dataset\n",
        "test_predictions = best_model.predict(test_preprocessed)\n",
        "\n",
        "# Create submission DataFrame\n",
        "# Replace 'Id' with the correct identifier column from your test dataset\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'SalePrice': test_predictions\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file for submission\n",
        "submission.to_csv('house_prices_submission_cat_improved.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
