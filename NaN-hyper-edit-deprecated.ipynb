{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dQZL_grBP1qP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, Ridge, Lasso\n",
        "from math import sqrt\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import uniform, randint\n",
        "from enum import Enum\n",
        "from itertools import product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uWXFvqnLP1qT"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "train_df = pd.read_csv('datasets/train.csv')\n",
        "test_df = pd.read_csv('datasets/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCnAJc49P1qX",
        "outputId": "0de4458f-6330-40a1-9346-8eb798ba8a99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression RMSE: 59738.63561964673\n",
            "Ridge Regression RMSE: 47251.178315544945\n",
            "Lasso Regression RMSE: 56726.558615929185\n",
            "Decision Tree RMSE: 52821.18414518933\n",
            "Random Forest RMSE: 34580.02593951611\n",
            "Trying LotFrontage NaN Handler... (1 - 5) : (<NaNSolution.DEFAULT: 1>,)\n",
            "Linear Regression RMSE: 59738.63561964673\n",
            "Ridge Regression RMSE: 47251.178315544945\n",
            "Lasso Regression RMSE: 56726.558615929185\n",
            "Decision Tree RMSE: 52821.18414518933\n",
            "Random Forest RMSE: 34580.02593951611\n",
            "Trying LotFrontage NaN Handler... (1 - 5) : (<NaNSolution.MEDIAN: 2>,)\n",
            "Linear Regression RMSE: 59738.63561964673\n",
            "Ridge Regression RMSE: 47251.178315544945\n",
            "Lasso Regression RMSE: 56726.558615929185\n",
            "Decision Tree RMSE: 52821.18414518933\n",
            "Random Forest RMSE: 34580.02593951611\n",
            "Trying LotFrontage NaN Handler... (1 - 5) : (<NaNSolution.MEAN: 3>,)\n",
            "Linear Regression RMSE: 59738.63561964673\n",
            "Ridge Regression RMSE: 47251.178315544945\n",
            "Lasso Regression RMSE: 56726.558615929185\n",
            "Decision Tree RMSE: 52821.18414518933\n",
            "Random Forest RMSE: 34580.02593951611\n",
            "Trying LotFrontage NaN Handler... (1 - 5) : (<NaNSolution.DROP: 4>,)\n",
            "Linear Regression RMSE: 59738.63561964673\n",
            "Ridge Regression RMSE: 47251.178315544945\n",
            "Lasso Regression RMSE: 56726.558615929185\n",
            "Decision Tree RMSE: 52821.18414518933\n",
            "Random Forest RMSE: 34580.02593951611\n",
            "Trying LotFrontage NaN Handler... (1 - 5) : (<NaNSolution.INTERPOLATE: 5>,)\n",
            "Best Model Score                     : 0.9795995099696784\n",
            "\n",
            "\n",
            "Lowest RSME                      : 34580.02593951611\n",
            "Best Regressor                       : Forest\n",
            "Best LotFrontage NaN Handler (1 - 5) : (<NaNSolution.DEFAULT: 1>,)\n"
          ]
        }
      ],
      "source": [
        "# NAN Solver : Really big block of spaghetti code\n",
        "\n",
        "# defined different methods of handling nans:\n",
        "# 1. set zero or 'vanilla' value\n",
        "# 2. fill median\n",
        "# 3. fill average\n",
        "# 4. drop\n",
        "# 5. interpolate\n",
        "\n",
        "class NaNSolution(Enum):\n",
        "    DEFAULT = 1\n",
        "    MEDIAN = 2\n",
        "    MEAN = 3\n",
        "    DROP = 4\n",
        "    INTERPOLATE = 5\n",
        "    \n",
        "# Refactor\n",
        "def update_best_model(model_name, new_rmse, new_score, flag_LotFrontage):\n",
        "    global least_rmse, best_score, best_regressor, best_flag_LotFrontage\n",
        "    if new_rmse < least_rmse:\n",
        "        least_rmse = new_rmse\n",
        "        best_score = new_score\n",
        "        best_regressor = model_name\n",
        "        best_flag_LotFrontage = flag_LotFrontage\n",
        "\n",
        "# Create permutations\n",
        "permutations = product(NaNSolution, repeat=1)  # Adjust 'repeat' as needed\n",
        "\n",
        "best_score = 0\n",
        "# remember best RSME result :\n",
        "least_rmse  = 9999999\n",
        "\n",
        "# best score NaN handler configuration :\n",
        "best_flag_LotFrontage = NaNSolution.DEFAULT\n",
        "best_flag_MasVnrType = NaNSolution.DEFAULT\n",
        "best_regressor = \"None\"\n",
        "\n",
        "# ====================================================================\n",
        "# Iterate over permutations\n",
        "for permutation in permutations:\n",
        "    \n",
        "    # Resetting the dataframes\n",
        "    permute_train_df = train_df.copy()\n",
        "    permute_test_df = test_df.copy()\n",
        "    \n",
        "    flag_LotFrontage = permutation\n",
        "\n",
        "    # reload data each permutation :\n",
        "    permute_train_df = train_df\n",
        "    permute_test_df = test_df\n",
        "\n",
        "    match flag_LotFrontage:\n",
        "        case NaNSolution.DEFAULT:\n",
        "        # default :\n",
        "            permute_train_df['LotFrontage'] = permute_train_df['LotFrontage'].fillna(0)\n",
        "\n",
        "        case NaNSolution.MEDIAN:\n",
        "            permute_train_df['LotFrontage'] = permute_train_df['LotFrontage'].fillna(permute_train_df.median())\n",
        "\n",
        "        case NaNSolution.MEAN:\n",
        "            permute_train_df['LotFrontage'] = permute_train_df['LotFrontage'].fillna(permute_train_df.mean())\n",
        "\n",
        "        case NaNSolution.DROP:\n",
        "            permute_train_df['LotFrontage'] = permute_train_df['LotFrontage'].dropna()\n",
        "\n",
        "        case NaNSolution.INTERPOLATE:\n",
        "            permute_train_df['LotFrontage'] = permute_train_df['LotFrontage'].interpolate(method='linear', limit_direction='forward', axis=0)\n",
        "\n",
        "    # -------------------------------\n",
        "    X = permute_train_df.drop('SalePrice', axis=1)\n",
        "    y = permute_train_df['SalePrice']\n",
        "\n",
        "    # Lists of numerical and categorical columns\n",
        "    numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
        "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() < 10]\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    numerical_transformer = SimpleImputer(strategy='median') # NaNs should already by filled by this point (at least for most significant columns), hopefully.\n",
        "\n",
        "    # Preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Applying the preprocessing transformations\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split the preprocessed data into training and validation sets\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_preprocessed, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "    # -------------------------------\n",
        "    # Separating target variable and predictors\n",
        "    X = permute_train_df.drop('SalePrice', axis=1)\n",
        "    y = permute_train_df['SalePrice']\n",
        "\n",
        "    # Removing columns with too many missing values (>50% missing)\n",
        "    too_many_missing = [col for col in X.columns if X[col].isnull().sum() > X.shape[0] * 0.5]\n",
        "    X.drop(too_many_missing, axis=1, inplace=True)\n",
        "\n",
        "    # Lists of numerical and categorical columns\n",
        "    numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64']]\n",
        "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object' and X[col].nunique() < 10]\n",
        "\n",
        "    # Preprocessing for numerical data\n",
        "    numerical_transformer = SimpleImputer(strategy='median')\n",
        "\n",
        "    # Preprocessing for categorical data\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Bundle preprocessing for numerical and categorical data\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Applying the preprocessing transformations\n",
        "    X_preprocessed = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Split the preprocessed data into training and validation sets\n",
        "    X_train, X_valid, y_train, y_valid = train_test_split(X_preprocessed, y, train_size=0.8, test_size=0.2, random_state=0)\n",
        "\n",
        "      # -----------------------------------------------\n",
        "\n",
        "    # Define and train models - ** TODO : still need to tune these systematically **\n",
        "    model_linear = LinearRegression()\n",
        "    model_ridge_cv = RidgeCV(alphas=[0.01, 0.1, 1, 10, 100], cv=5)\n",
        "    model_lasso_cv = LassoCV(alphas=[0.0001, 0.001, 0.01, 0.1, 1, 10], cv=5, max_iter=100000)\n",
        "    model_tree = DecisionTreeRegressor(random_state=0)\n",
        "    model_forest = RandomForestRegressor(n_estimators=100, random_state=0)\n",
        "    \n",
        "    # Define the parameter distribution for each model\n",
        "    param_dist_ridge = {'alpha': uniform(0.01, 100)}\n",
        "    param_dist_lasso = {'alpha': uniform(0.0001, 10)}\n",
        "    param_dist_tree = {'max_depth': [None] + list(range(1, 31)), 'min_samples_split': randint(2, 11)}\n",
        "    param_dist_forest = {'n_estimators': randint(50, 200), 'max_depth': [None] + list(range(1, 31))}\n",
        "\n",
        "    # Create a RandomizedSearchCV object for each model\n",
        "    rand_ridge = RandomizedSearchCV(Ridge(), param_distributions=param_dist_ridge, n_iter=100, cv=5)\n",
        "    rand_ridge.fit(X_train, y_train)\n",
        "    rand_lasso = RandomizedSearchCV(Lasso(max_iter=100000), param_distributions=param_dist_lasso, n_iter=100, cv=5)\n",
        "    rand_tree = RandomizedSearchCV(DecisionTreeRegressor(random_state=0), param_distributions=param_dist_tree, n_iter=100, cv=5)\n",
        "    rand_forest = RandomizedSearchCV(RandomForestRegressor(random_state=0), param_distributions=param_dist_forest, n_iter=100, cv=5)\n",
        "\n",
        "    # Fit models\n",
        "    model_linear.fit(X_train, y_train)\n",
        "    model_ridge_cv.fit(X_train, y_train)\n",
        "    model_lasso_cv.fit(X_train, y_train)\n",
        "    model_tree.fit(X_train, y_train)\n",
        "    model_forest.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions and RMSE\n",
        "    predictions_linear = model_linear.predict(X_valid)\n",
        "    predictions_ridge = model_ridge_cv.predict(X_valid)\n",
        "    predictions_lasso = model_lasso_cv.predict(X_valid)\n",
        "    predictions_tree = model_tree.predict(X_valid)\n",
        "    predictions_forest = model_forest.predict(X_valid)\n",
        "\n",
        "    rmse_linear = sqrt(mean_squared_error(y_valid, predictions_linear))\n",
        "    rmse_ridge = sqrt(mean_squared_error(y_valid, predictions_ridge))\n",
        "    rmse_lasso = sqrt(mean_squared_error(y_valid, predictions_lasso))\n",
        "    rmse_tree = sqrt(mean_squared_error(y_valid, predictions_tree))\n",
        "    rmse_forest = sqrt(mean_squared_error(y_valid, predictions_forest))\n",
        "\n",
        "    print(f'Linear Regression RMSE: {rmse_linear}')\n",
        "    print(f'Ridge Regression RMSE: {rmse_ridge}')\n",
        "    print(f'Lasso Regression RMSE: {rmse_lasso}')\n",
        "    print(f'Decision Tree RMSE: {rmse_tree}')\n",
        "    print(f'Random Forest RMSE: {rmse_forest}')\n",
        "\n",
        "      # Lastly, compare with best score, save configuration for best score.\n",
        "\n",
        "    new_high_score = False\n",
        "\n",
        "    # Check and update for Linear model\n",
        "    update_best_model(\"Linear\", rmse_linear, model_linear.score(X_train, y_train), flag_LotFrontage)\n",
        "\n",
        "    # Check and update for Ridge model\n",
        "    update_best_model(\"Ridge\", rmse_ridge, model_ridge_cv.score(X_train, y_train), flag_LotFrontage)\n",
        "\n",
        "    # Check and update for Lasso model\n",
        "    update_best_model(\"Lasso\", rmse_lasso, model_lasso_cv.score(X_train, y_train), flag_LotFrontage)\n",
        "\n",
        "    # Check and update for Decision Tree model\n",
        "    update_best_model(\"Tree\", rmse_tree, model_tree.score(X_train, y_train), flag_LotFrontage)\n",
        "\n",
        "    # Check and update for Random Forest model\n",
        "    update_best_model(\"Forest\", rmse_forest, model_forest.score(X_train, y_train), flag_LotFrontage)\n",
        "\n",
        "    if new_high_score == True:\n",
        "        best_flag_LotFrontage = flag_LotFrontage\n",
        "\n",
        "    print(f'Trying LotFrontage NaN Handler... (1 - 5) : {flag_LotFrontage}')\n",
        "\n",
        "# print result\n",
        "print(f'Best Model Score                     : {best_score}')\n",
        "print(f'\\n\\nLowest RSME                      : {least_rmse}')\n",
        "print(f'Best Regressor                       : {best_regressor}')\n",
        "print(f'Best LotFrontage NaN Handler (1 - 5) : {best_flag_LotFrontage}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\maxim\\OneDrive\\Desktop\\cis-assignment2\\CIS400-Group-Project\\NaN-hyper-edit.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maxim/OneDrive/Desktop/cis-assignment2/CIS400-Group-Project/NaN-hyper-edit.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_full \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mfit_transform(train_df\u001b[39m.\u001b[39mdrop(\u001b[39m'\u001b[39m\u001b[39mSalePrice\u001b[39m\u001b[39m'\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maxim/OneDrive/Desktop/cis-assignment2/CIS400-Group-Project/NaN-hyper-edit.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m y_full \u001b[39m=\u001b[39m train_df[\u001b[39m'\u001b[39m\u001b[39mSalePrice\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/maxim/OneDrive/Desktop/cis-assignment2/CIS400-Group-Project/NaN-hyper-edit.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m best_model\u001b[39m.\u001b[39;49mfit(X_full, y_full)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maxim/OneDrive/Desktop/cis-assignment2/CIS400-Group-Project/NaN-hyper-edit.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Predict house prices on the test dataset\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/maxim/OneDrive/Desktop/cis-assignment2/CIS400-Group-Project/NaN-hyper-edit.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m test_preprocessed \u001b[39m=\u001b[39m preprocessor\u001b[39m.\u001b[39mtransform(test_df)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1809\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1808\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1809\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1810\u001b[0m         ParameterSampler(\n\u001b[0;32m   1811\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1812\u001b[0m         )\n\u001b[0;32m   1813\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    727\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    728\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 729\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[0;32m    731\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    732\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1320\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m   1291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m   1292\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \n\u001b[0;32m   1294\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1317\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1318\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1320\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[0;32m   1321\u001b[0m         X,\n\u001b[0;32m   1322\u001b[0m         y,\n\u001b[0;32m   1323\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1324\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1325\u001b[0m     )\n\u001b[0;32m   1326\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\maxim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\tree\\_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    434\u001b[0m         splitter,\n\u001b[0;32m    435\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    441\u001b[0m     )\n\u001b[1;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[0;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_model = rand_forest\n",
        "\n",
        "# Fit the model on the entire training dataset\n",
        "X_full = preprocessor.fit_transform(train_df.drop('SalePrice', axis=1))\n",
        "y_full = train_df['SalePrice']\n",
        "best_model.fit(X_full, y_full)\n",
        "\n",
        "# Predict house prices on the test dataset\n",
        "test_preprocessed = preprocessor.transform(test_df)\n",
        "test_predictions = best_model.predict(test_preprocessed)\n",
        "\n",
        "# Create submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'Id': test_df['Id'],\n",
        "    'SalePrice': test_predictions\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file for submission\n",
        "submission.to_csv('house_prices_submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
